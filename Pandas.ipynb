{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55601b3d-cd9f-4c13-9eda-8cd27f54202c",
   "metadata": {},
   "source": [
    "## How to Interact with this Jupyter Notebook\n",
    "\n",
    "In this activity, you will use a Jupyter Notebook, which integrates both text and code. The gray boxes contain executable code, which you will run in order to view its output. The text in between the code provides instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af126b-b291-4ae1-8cf8-f3883f7195aa",
   "metadata": {},
   "source": [
    "## Scenario: Charting the Customer Journey with Pandas\n",
    "\n",
    "Imagine you're a Python developer at a rapidly growing e-commerce company. The marketing team is eager to understand customer behavior and preferences to tailor their campaigns and improve the overall shopping experience. They've provided you with a valuable dataset containing information about customers, their purchases, and demographics. \n",
    "\n",
    "Your task is to leverage your Python skills and the power of the Pandas library to load this dataset, explore its structure, and uncover preliminary insights that will guide further analysis. This initial exploration is crucial for understanding the data you're working with and making informed decisions about how to proceed with more in-depth analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc3f26",
   "metadata": {},
   "source": [
    "In the cell below, begin by importing the `pandas` library with the alias `pd`. Then, use `.read_csv()` to load the `customer_data_50.csv` file into a DataFrame named `customer_data`. \n",
    "\n",
    "Lastly, run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7c8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library with the alias 'pd'\n",
    "\n",
    "# insert code here \n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file 'customer_data_50.csv' into a DataFrame\n",
    "\n",
    "# insert code here \n",
    "customer_data = pd.read_csv('customer_data_50.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f7a73-2b4e-4d4d-8018-c6ee38acab95",
   "metadata": {},
   "source": [
    "Run the following cell, which will check the dimensions of your DataFrame using the `.shape` attribute. This tells you how many rows and columns your data has â€“ kind of like figuring out the size of a spreadsheet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0249d4-c0dd-4cba-8f4b-070130e8f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the DataFrame (rows, columns): (50, 13)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the DataFrame (rows, columns)\n",
    "print(\"\\nShape of the DataFrame (rows, columns):\", customer_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7982d00-92d7-4db5-b529-37aa53d03b0b",
   "metadata": {},
   "source": [
    "Next, you'll inspect the data using the `df.head()` function, which allows you to view the first few rows of the DataFrame. This gives you a quick look at the data's structure and content.\n",
    "\n",
    "In the cell below, use `df.head()`to display the first 5 rows of the `customer_data` DataFrame.  Then, run the cell and take a moment to observe the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e941605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "\n",
      "   customer_id first_name  last_name                       email gender  age  \\\n",
      "0         1001     Sophia      Smith    sophia.smith@example.com      M   54   \n",
      "1         1002     Joseph      Smith    joseph.smith@example.com      M   66   \n",
      "2         1003       John   Anderson   john.anderson@example.com      F   56   \n",
      "3         1004       Emma  Hernandez  emma.hernandez@example.com      M   44   \n",
      "4         1005      Emily     Garcia    emily.garcia@example.com      F   25   \n",
      "\n",
      "          city state country  purchase_count  total_spend  avg_order_value  \\\n",
      "0  San Antonio    TX     USA               5          965            193.0   \n",
      "1  Los Angeles    CA     USA               7         1246            178.0   \n",
      "2      Phoenix    AZ     USA               1          199            199.0   \n",
      "3  Los Angeles    CA     USA              14         3752            268.0   \n",
      "4       Dallas    TX     USA              12         1620            135.0   \n",
      "\n",
      "           last_purchase_date  \n",
      "0  2023-09-12 15:28:32.140488  \n",
      "1  2023-08-25 15:28:32.140488  \n",
      "2  2024-04-28 15:28:32.140488  \n",
      "3  2024-01-01 15:28:32.140488  \n",
      "4  2023-12-09 15:28:32.140488  \n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows\n",
    "print(\"First 5 rows:\\n\")\n",
    "\n",
    "# insert code here \n",
    "print(customer_data.head()) #delete this adeeb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d50c2-5e1e-46ee-b2f0-d43f2d631d8d",
   "metadata": {},
   "source": [
    "Now, you'll use the `df.info()` function, which provides a concise summary of the DataFrame, including the column names, their data types, and the number of non-null values.\n",
    "\n",
    "In the cell below, use `df.info()` to print information about the `customer_data` DataFrame  Then, run the cell and take a moment to observe the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933fbc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names and their data types:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   customer_id         50 non-null     int64  \n",
      " 1   first_name          50 non-null     object \n",
      " 2   last_name           50 non-null     object \n",
      " 3   email               50 non-null     object \n",
      " 4   gender              50 non-null     object \n",
      " 5   age                 50 non-null     int64  \n",
      " 6   city                50 non-null     object \n",
      " 7   state               50 non-null     object \n",
      " 8   country             50 non-null     object \n",
      " 9   purchase_count      50 non-null     int64  \n",
      " 10  total_spend         50 non-null     int64  \n",
      " 11  avg_order_value     50 non-null     float64\n",
      " 12  last_purchase_date  50 non-null     object \n",
      "dtypes: float64(1), int64(4), object(8)\n",
      "memory usage: 5.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the column names and their data types\n",
    "print(\"\\nColumn names and their data types:\\n\")\n",
    "\n",
    "# insert code here \n",
    "print(customer_data.info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647fd7c-e95c-4647-9770-d08ae34f3f35",
   "metadata": {},
   "source": [
    "Next, you'll use the `df.describe()` function, which generates descriptive statistics for the numerical columns in the DataFrame.\n",
    "\n",
    "In the cell below, use `df.describe()` to display summary statistics for the numerical columns in the `customer_data` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59033260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive statistics for numerical columns:\n",
      "\n",
      "       customer_id        age  purchase_count  total_spend  avg_order_value\n",
      "count     50.00000  50.000000        50.00000    50.000000        50.000000\n",
      "mean    1025.50000  43.440000         8.60000  1491.880000       179.920000\n",
      "std       14.57738  14.833993         4.28095   968.697666        70.820221\n",
      "min     1001.00000  19.000000         1.00000   199.000000        53.000000\n",
      "25%     1013.25000  30.000000         5.00000   819.000000       125.750000\n",
      "50%     1025.50000  44.500000         8.00000  1350.000000       180.000000\n",
      "75%     1037.75000  54.000000        12.00000  1916.000000       237.500000\n",
      "max     1050.00000  69.000000        15.00000  4440.000000       299.000000\n"
     ]
    }
   ],
   "source": [
    "# Display descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive statistics for numerical columns:\\n\")\n",
    "\n",
    "# insert code here \n",
    "print(customer_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ed4d5-9198-4bfd-a26f-e239c0246d0b",
   "metadata": {},
   "source": [
    "Finally, in the code cell below, you'll use the `.mean()` and `.median()` functions on the `'age'` column of your `customer_data` to calculate the average and median age of all your customers. \n",
    "\n",
    "The square brackets [] are used for column selection in Pandas. Within the brackets, you specify the name of the column you want to extract, which in this case is 'age'\n",
    "\n",
    "Run the cell to see the average and median age of your customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c7951d-733d-4661-a012-4c020ba3d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Age: 43.44\n",
      "\n",
      "Median Age: 44.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of the 'age' column\n",
    "mean_age = customer_data['age'].mean() # insert code here \n",
    "\n",
    "# Print the mean age\n",
    "print(\"\\nMean Age:\", mean_age)\n",
    "\n",
    "# Calculate the median of the 'age' column\n",
    "median_age = customer_data['age'].median() # insert code here \n",
    "\n",
    "# Print the median age\n",
    "print(\"\\nMedian Age:\", median_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b7f55",
   "metadata": {},
   "source": [
    "## Activity Recap: Charting the Customer Journey with Pandas\n",
    "\n",
    "Congratulations! In this activity, you learned how to load a CSV file into a Pandas DataFrame and use various functions to inspect its structure and contents:\n",
    "\n",
    "* `pd.read_csv()` is used to load CSV data into a DataFrame.\n",
    "* `df.head()` shows the first few rows.\n",
    "* `df.info()` provides a summary of the DataFrame's structure.\n",
    "* `df.describe()` generates descriptive statistics for numerical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2fcb4-6bce-4613-8450-a3556856e68d",
   "metadata": {},
   "source": [
    "# pandas cheat sheet\n",
    "In data science and analysis, pandas is an indispensable library and a great toolkit for data wrangling. Its ability to handle, clean, transform, and analyze data with remarkable efficiency has cemented its position as a cornerstone for both aspiring and seasoned professionals in the field. Consider pandas your trusted companion, because it has a rich array of functions and syntax that unlock hidden narratives within datasets. From completing basic operations to intricate transformations, this cheat sheet guides you through data manipulation with confidence.\n",
    "\n",
    "## Creating and inspecting DataFrames\n",
    "At the heart of pandas lies the DataFrame, a powerful data structure organizing data into neat rows and columns, mirroring how we conceptualize information. Think of it like a spreadsheet where each row represents a unique entry where these entries could be a person or a transaction, and each column represents a specific characteristic of those entries like age, name, or purchase amount. pandas offers diverse ways to create DataFrames, accommodating various data sources. These sources can come from dictionaries, lists of lists, and CSV files.\n",
    "\n",
    "* Dictionaries * are ideal for structured data, where keys represent column names and values correspond to column data, streamlining DataFrame creation from pre-organized data. This is particularly useful when you have data stored in a dictionary format, perhaps after processing JSON data or extracting information from a database.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 28]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "List of lists are convenient for tabular data. Each inner list represents a row, and an optional columns argument specifies column names, ensuring clarity and interpretability. This method is handy when you have data organized in a list-of-lists format, perhaps after reading data from a text file or scraping a website.\n",
    "\n",
    "data = [['Alice', 25], ['Bob', 30], ['Charlie', 28]]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "CSV files give pandas the ability to seamlessly load data from them into DataFrames using the read_csv function. This function handles parsing and structuring. CSV files are a common format for storing and exchanging data, and pandas makes it effortless to import them into your Python environment for analysis.\n",
    "\n",
    "- df = pd.read_csv('data.csv')\n",
    "\n",
    "pandas also provides similar functions like read_excel, read_json, and read_sql for importing data from other popular formats, making it adaptable to your specific data sources.\n",
    "\n",
    "## Inspecting a DataFrame\n",
    "Once you have your DataFrame ready, pandas provides a suite of functions to explore and understand its structure and content:\n",
    "\n",
    "- df.head(): Displays the first few rows (default 5) for a quick overview of the data's nature, including column names, data types, and a sample of values. It's like taking a peek at the beginning of a dataset to get a feel for what it contains.\n",
    "\n",
    "- df.tail(): Similarly, displays the last few rows (default 5), aiding in identifying patterns or anomalies towards the end. This can be helpful in spotting any unexpected values or trends that might require further investigation.\n",
    "\n",
    "- df.shape: Returns a tuple representing the dimensions of the DataFrame (number of rows and columns). This is crucial for understanding the scale of your data and planning your analysis strategy accordingly.\n",
    "\n",
    "- df.info(): Generates a concise summary of the DataFrame, encompassing column names, data types (essential for knowing how to work with each column), and the presence of missing values. This summary provides a quick health check of your data, highlighting potential areas for cleaning or preprocessing.\n",
    "\n",
    "- df.describe(): Computes descriptive statistics for numerical columns, offering insights into data distribution and central tendencies. This includes metrics like count, mean, standard deviation, minimum, maximum, and quartiles, which help you understand the characteristics of your numerical data.\n",
    "\n",
    "## Selecting and filtering data\n",
    "The true power of pandas lies in its ability to precisely select and filter data, allowing you to focus your analysis on specific subsets of interest. This capability is fundamental for extracting meaningful information from large and complex datasets. pandas provides intuitive syntax for column selection.\n",
    "\n",
    "Single column selection retrieves a single column by its name using square brackets. The result is a pandas Series, a one-dimensional labeled array capable of holding any data type. Series objects inherit many of the DataFrame's functionalities, enabling further manipulation and analysis on a single column.\n",
    "\n",
    "df['Age'] \n",
    "\n",
    "Multiple column selection extracts multiple columns by passing a list of column names within double square brackets. The output is a new DataFrame containing only the selected columns, effectively creating a focused subset of the original data. This is useful when you want to work with a specific group of variables or features.\n",
    "\n",
    "df[['Name', 'Age']] \n",
    "\n",
    "pandas offers two primary methods for row selection, each tailored to specific indexing needs.\n",
    "\n",
    "- df.loc[] employs label-based indexing, allowing you to select rows based on their index labels, which can be strings, integers, or even dates. This is particularly helpful when your DataFrame has meaningful index labels that you want to use for selection.\n",
    "\n",
    "- df.loc[0]       # Select the row with index label 0\n",
    "\n",
    "- df.iloc[] leverages integer-based indexing, enabling row selection based on their integer positions (starting from 0). This is useful when you want to select rows based on their order in the DataFrame, regardless of their index labels.\n",
    "\n",
    "- df.iloc[0]      # Select the first row (position 0)\n",
    "\n",
    "Filtering data is done with Boolean indexing and serves as a powerful tool for filtering rows based on specific conditions. It involves creating a boolean mask (a Series of True/False values) by applying comparison operators or logical conditions to one or more columns. This mask is then used to select only the rows where the condition evaluates to True, effectively filtering the DataFrame based on your criteria. This allows you to extract subsets of data that meet specific requirements, facilitating targeted analysis.\n",
    "\n",
    "df[df['Age'] > 25]  # Filter rows where 'Age' is greater than 25\n",
    "\n",
    "Querying data ### is for more complex filtering scenarios involving multiple conditions or intricate logic. The query method offers a SQL-like syntax. This enhances readability and expressiveness, allowing you to construct queries that resemble natural language expressions, making your code more intuitive and maintainable. It's particularly beneficial when dealing with intricate filtering criteria that would be cumbersome to express using traditional boolean indexing.\n",
    "\n",
    "df.query('Age > 25 and Name == \"Bob\"') # Filter rows based on multiple conditions\n",
    "\n",
    "## Handling missing data\n",
    "Real-world datasets are rarely perfect; missing values, denoted as NaN, are common to find. These missing values can come from various sources, such as data entry errors, sensor malfunctions, or incomplete surveys. pandas provides robust mechanisms for identifying and addressing these missing values, ensuring the integrity of your analysis and preventing potential errors or biases that can skew your results.\n",
    "\n",
    "df.isnull() generates a boolean DataFrame mirroring the original, where True indicates a missing value and False represents a non-missing value. This provides a comprehensive overview of the missing data landscape within your DataFrame, allowing you to quickly assess the extent of what is missing and identify columns or rows that require attention.\n",
    "\n",
    "df['Age'].isnull() applies the same logic to a specific column, returning a boolean series highlighting missing values within that column. This allows you to focus your attention on specific variables and assess for their completeness, which is important for deciding on appropriate imputation strategies.\n",
    "\n",
    "pandas offers a spectrum of strategies for handling missing data, each tailored to specific scenarios and analytical objectives:\n",
    "\n",
    "df.dropna() removes rows containing any missing values, effectively shrinking the DataFrame but ensuring data completeness. This approach is suitable when missing values are relatively sparse and their removal doesn't significantly impact the representativeness of the data. However, it's important to exercise caution, as dropping rows can lead to loss of valuable information if not done judiciously.\n",
    "\n",
    "df.fillna(0) replaces missing values with a specified value (e.g., 0), a simple imputation technique suitable for certain cases. This can be useful when you have a reasonable assumption about the likely value of missing data or when you want to avoid discarding rows altogether. However, it's important to consider the potential implications of this imputation on your analysis, as it might introduce bias or distort the underlying patterns in the data.\n",
    "\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True) is a more sophisticated approach, imputing missing values in a column with its mean. The inplace=True argument modifies the DataFrame directly, conserving memory. This method is often preferred when missing values are assumed to be randomly distributed and you want to maintain the overall statistical properties of the data\n",
    "\n",
    "It's important to be aware that mean imputation can underestimate variance and potentially mask underlying patterns in the data. Other imputation techniques, such as median imputation or more advanced methods like regression imputation or multiple imputation, might be more appropriate depending on the specific characteristics of your dataset and the nature of what is missing.\n",
    "\n",
    "pandas is an important tool for anyone navigating the world of data. It offers a comprehensive suite of functions and syntax to streamline data manipulation tasks. By learning these functions, you'll be well-equipped to confront diverse data challenges and unveil valuable insights while you work. Remember, practice is key to solidifying your understanding and proficiency with these tools. Explore real-world datasets and spend time experimenting with all the possibilities that pandas offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb54ae6-5a77-42c4-a073-3e39b04f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'Name': ['Alice', 'Bob', np.nan, 'David'], \n",
    "        'Age': [25, 30, np.nan, 35], \n",
    "        'City': ['New York', np.nan, 'London', 'Paris']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Identifying missing values\n",
    "print(\"Missing value counts per column:\\n\", df.isnull().sum())\n",
    "\n",
    "# 2. Removing missing values (dropna)\n",
    "df_dropped = df.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with any missing value:\\n\", df_dropped)\n",
    "\n",
    "# 3. Imputing with mean (for numerical columns)\n",
    "df_filled_mean = df.fillna(df.mean(numeric_only=True))\n",
    "print(\"\\nDataFrame after filling missing 'Age' with mean:\\n\", df_filled_mean)\n",
    "\n",
    "# 3. Imputing with median (for numerical columns)\n",
    "df_filled_median = df.fillna(df.median(numeric_only=True))\n",
    "print(\"\\nDataFrame after filling missing 'Age' with median:\\n\", df_filled_median)\n",
    "\n",
    "# 4. Handling outliers (demonstration with 'Age')\n",
    "# Assuming we identify 40 as an outlier based on domain knowledge or visualization\n",
    "df['Age_capped'] = df['Age'].clip(upper=40)  # Cap values at 28\n",
    "print(\"\\nDataFrame with 'Age' capped at 40:\\n\", df)\n",
    "\n",
    "# 5. Data type conversion\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')  # Convert to numeric, handling errors\n",
    "print(\"\\nData types after conversion:\\n\", df.dtypes)\n",
    "\n",
    "# 6. Exploratory Data Analysis\n",
    "print(\"\\nDescriptive statistics:\\n\", df.describe())\n",
    "\n",
    "# Group by and aggregate\n",
    "grouped_data = df.groupby('City')['Age'].mean()\n",
    "print(\"\\nAverage Age by City:\\n\", grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbd727-f152-4519-ac65-efa6d58c2793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
